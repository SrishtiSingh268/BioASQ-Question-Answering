{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'nli/infersent/'\n",
      "/Users/nitish/Documents/Box Sync/cmu/acads/11797/bioasq/nli/infersent\n"
     ]
    }
   ],
   "source": [
    "cd nli/infersent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "GLOVE_PATH = '/Volumes/Nitish-Passport/bioasq_files/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'saved_models/infersent.allnli.pickle'\n",
    "model = torch.load(model_path, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_glove_path(GLOVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'This is a sample sentence.',\n",
    "    'The cat likes to walk.',\n",
    "    'The cat likes to talk.',\n",
    "    'The dog likes to talk.',\n",
    "    'This is a completely unrelated sentence.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17(/17) words with glove vectors\n",
      "Vocab size : 17\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb words kept : 31/36 (86.11 %)\n",
      "Speed : 7.6 sentences/s (cpu mode, bsize=2)\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(sentences, bsize=2, tokenize=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4096)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "def sim(sent1, sent2):\n",
    "    em = model.encode([sent1, sent2], tokenize=False, verbose=True)\n",
    "    return cosine(em[0], em[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'sentence',\n",
       " 'This',\n",
       " 'is',\n",
       " 'walk',\n",
       " 'dog',\n",
       " '.',\n",
       " 'sample',\n",
       " 'to',\n",
       " 'completely',\n",
       " 'unrelated',\n",
       " 'likes',\n",
       " 'The',\n",
       " 'cat',\n",
       " '</s>',\n",
       " 'talk',\n",
       " '<s>']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_word_dict(sentences).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "run data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "GLOVE_PATH = '/Volumes/Nitish-Passport/bioasq_files/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = [\n",
    "    'I do not want to go home.',\n",
    "    'He likes butter.',\n",
    "    'He never speaks loud.',\n",
    "    'The weather is good',\n",
    "    'What is the matter?',\n",
    "    'He never speaks loud.',\n",
    "    'He made two mistakes',\n",
    "]\n",
    "s2 = [\n",
    "    'I wish to go home.',\n",
    "    'He hates butter.',\n",
    "    'He never speaks soft.',\n",
    "    'The weather is bad',\n",
    "    'What is the matter with you?',\n",
    "    'He always speaks soft.',\n",
    "    'He made mistakes twice.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36(/36) words with glove vectors\n"
     ]
    }
   ],
   "source": [
    "word_dict = get_word_dict(s1 + s2)\n",
    "word_vec = get_glove(word_dict, GLOVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 0, 2, 0, 2, 0]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_net = torch.load('saved_models/nlimodels/model.pickle', map_location='cpu')\n",
    "nli_net.eval()\n",
    "outputs = []\n",
    "for i in range(len(s1)):\n",
    "    ss1 = [['<s>'] + [word for word in word_tokenize(sent) if word in word_vec] +  ['</s>'] for sent in [s1[i]]]\n",
    "    ss2 = [['<s>'] + [word for word in word_tokenize(sent) if word in word_vec] +  ['</s>'] for sent in [s2[i]]]\n",
    "    # ss1 = sorted(ss1, key = lambda x: len(x), reverse=True)\n",
    "    # ss2 = sorted(ss2, key = lambda x: len(x), reverse=True)\n",
    "    s1_batch, s1_len = get_batch(ss1, word_vec)\n",
    "    s2_batch, s2_len = get_batch(ss2, word_vec)\n",
    "    s1_batch, s2_batch = Variable(s1_batch), Variable(s2_batch)\n",
    "    output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "    outputs.append(output.data.max(1)[1].numpy()[0])\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I do not want to go home.',\n",
       "  'He likes butter.',\n",
       "  'He never speaks loud.',\n",
       "  'The weather is good',\n",
       "  'What is the matter?',\n",
       "  'He never speaks loud.',\n",
       "  'He made two mistakes'],\n",
       " ['I wish to go home.',\n",
       "  'He hates butter.',\n",
       "  'He never speaks soft.',\n",
       "  'The weather is bad',\n",
       "  'What is the matter with you?',\n",
       "  'He always speaks soft.',\n",
       "  'He made mistakes twice.'])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1, s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prostrcustus transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nitish/Documents/Box Sync/cmu/acads/11797/bioasq/nli/infersent/data\n"
     ]
    }
   ],
   "source": [
    "cd ~/acads/11797/bioasq/nli/infersent/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.load('matmul_glove_med.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, V = np.linalg.svd(mat, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 200), (200,), (200, 200))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape, s.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = U.dot(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('prostrcustus_transformation_matrix.npy', tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nitish/Documents/Box Sync/cmu/acads/11797/bioasq\n"
     ]
    }
   ],
   "source": [
    "cd ~/acads/11797/bioasq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataLoader import DataLoader\n",
    "\n",
    "input_path = 'input/BioASQ-trainingDataset6b.json'\n",
    "# input_path = 'input/BioASQ-task6bPhaseB-testset3.json'\n",
    "data = DataLoader(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_vectors_file = '/Volumes/Nitish-Passport/bioasq_files/wikipedia-pubmed-and-PMC-w2v.bin'\n",
    "\n",
    "input_glove_file = '/Volumes/Nitish-Passport/bioasq_files/glove.840B.300d.txt'\n",
    "new_glove_file = '/Volumes/Nitish-Passport/bioasq_files/%s_glove.840B.300d.txt' % data.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nitish/Documents/Box Sync/cmu/acads/11797/bioasq/nli\n"
     ]
    }
   ],
   "source": [
    "cd ~/acads/11797/bioasq/nli/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run hypotheses.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /Users/nitish/.local/share/bllipparser/GENIA+PubMed\n",
      "Model directory already exists, not reinstalling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 616/616 [01:21<00:00,  7.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# RerankingParser._parser_model_loaded = False\n",
    "# rrp = RerankingParser.from_unified_model_dir('/Users/nitish/.local/share/bllipparser/GENIA+PubMed')\n",
    "# # fetch_and_load('GENIA+PubMed', verbose=True)\n",
    "# yesno = data.get_questions_of_type('yesno')\n",
    "# for q in tqdm(yesno):\n",
    "#     q.assertion_pos = q2s(q.question, rrp)\n",
    "set_assertions_for_yesno_questions(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yesno = data.get_questions_of_type('yesno')\n",
    "all_sentences = []\n",
    "for q in yesno:\n",
    "    all_sentences.append(q.assertion_pos)\n",
    "    all_sentences += q.snippet_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nitish/Documents/Box Sync/cmu/acads/11797/bioasq/nli/infersent\n"
     ]
    }
   ],
   "source": [
    "cd ~/acads/11797/bioasq/nli/infersent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "run data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = get_word_dict(all_sentences)\n",
    "tokens = word_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "run create_glove_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pubtator_vectors = word2vec.load(pubmed_vectors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cPickle as pickle\n",
    "\n",
    "# with open('/Users/nitish/acads/11797/bioasq/nli/infersent/data/uncommon_vocab.pkl', 'rb') as fp:\n",
    "#     uncommon_vocab = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "with open(new_glove_file, 'w') as fp:\n",
    "    for word in tokens:\n",
    "        k += 1\n",
    "        if k % 5000 == 0:\n",
    "            print('Finished %d words' % k)\n",
    "        if word in uncommon_vocab and word in pubtator_vectors:\n",
    "            vec = tm.dot(pubtator_vectors[word][:, np.newaxis])[:, 0]\n",
    "            line = ' '.join([word] + [str(i) for i in vec])\n",
    "            fp.write('%s\\n' % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = tm.dot(pubtator_vectors['localizes'][:, np.newaxis])[:, 0].shape\n",
    "vec.lot_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run infersent on yesno questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nitish/Documents/Box Sync/cmu/acads/11797/bioasq\n"
     ]
    }
   ],
   "source": [
    "cd ~/acads/11797/bioasq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run dataLoader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'input/BioASQ-trainingDataset6b.json'\n",
    "# input_path = 'input/BioASQ-task6bPhaseB-testset3.json'\n",
    "data = DataLoader(input_path)\n",
    "\n",
    "input_glove_file = '/Volumes/Nitish-Passport/bioasq_files/glove.840B.300d.txt'\n",
    "new_glove_file = '/Volumes/Nitish-Passport/bioasq_files/%s_glove.840B.300d.txt' % data.name\n",
    "yesno_ans_file = 'output/yesno_%s.json' % data.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nitish/Documents/Box Sync/cmu/acads/11797/bioasq/nli\n"
     ]
    }
   ],
   "source": [
    "cd ~/acads/11797/bioasq/nli/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run hypotheses.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /Users/nitish/.local/share/bllipparser/GENIA+PubMed\n",
      "Model directory already exists, not reinstalling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 616/616 [01:36<00:00,  6.40it/s]\n"
     ]
    }
   ],
   "source": [
    "set_assertions_for_yesno_questions(data)\n",
    "\n",
    "yesno = data.get_questions_of_type('yesno')\n",
    "all_sentences = []\n",
    "for q in yesno:\n",
    "    all_sentences.append(q.assertion_pos)\n",
    "    all_sentences += q.snippet_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nitish/Documents/Box Sync/cmu/acads/11797/bioasq/nli/infersent\n"
     ]
    }
   ],
   "source": [
    "cd ~/acads/11797/bioasq/nli/infersent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "run data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22093(/23268) words with glove vectors\n"
     ]
    }
   ],
   "source": [
    "word_dict = get_word_dict(all_sentences)\n",
    "word_vec = get_glove(word_dict, input_glove_file, new_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0 yes 0\n"
     ]
    }
   ],
   "source": [
    "nli_net = torch.load('saved_models/nlimodels/model.pickle', map_location='cpu')\n",
    "nli_net.eval()\n",
    "\n",
    "for q in yesno[:1]:\n",
    "    outputs = []\n",
    "    sentences = [i['text'] for i in q.ranked_sentences()[:20]]\n",
    "    s1, s2 = sentences, [q.assertion_pos] * len(sentences)\n",
    "#     s2, s1 = sentences, [q.assertion_pos] * len(sentences)\n",
    "#     if q.exact_answer_ref == 'yes':\n",
    "#         continue\n",
    "    total = 0\n",
    "    for i in range(len(s1)):\n",
    "        ss1 = [['<s>'] + [word for word in word_tokenize(sent) if word in word_vec] +  ['</s>'] for sent in [s1[i]]]\n",
    "        ss2 = [['<s>'] + [word for word in word_tokenize(sent) if word in word_vec] +  ['</s>'] for sent in [s2[i]]]\n",
    "        # ss1 = sorted(ss1, key = lambda x: len(x), reverse=True)\n",
    "        # ss2 = sorted(ss2, key = lambda x: len(x), reverse=True)\n",
    "        s1_batch, s1_len = get_batch(ss1, word_vec)\n",
    "        s2_batch, s2_len = get_batch(ss2, word_vec)\n",
    "        s1_batch, s2_batch = Variable(s1_batch), Variable(s2_batch)\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "        total += output\n",
    "        outputs.append(output.data.max(1)[1].numpy()[0])\n",
    "    print (np.array(outputs) == 0).sum(), (np.array(outputs) == 2).sum(), q.exact_answer_ref, total.data.max(1)[1].numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.5178 -0.6057 -0.9143\n",
       "[torch.FloatTensor of size 1x3]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 616/616 [4:09:53<00:00, 24.34s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy = 0.590909 (364 / 616)\n",
      "YES Accuracy = 0.583969 (306 / 524)\n",
      "No Accuracy = 0.630435 (58 / 92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nli_net = torch.load('saved_models/nlimodels/model.pickle', map_location='cpu')\n",
    "nli_net.eval()\n",
    "\n",
    "def get_answer(n_entail, n_contradict):\n",
    "    if n_entail > n_contradict:\n",
    "        return 'yes'\n",
    "    elif n_contradict > n_entail:\n",
    "        return 'no'\n",
    "    else:\n",
    "        if n_entail == 0:\n",
    "            return 'no'\n",
    "        else:\n",
    "            return 'yes'\n",
    "\n",
    "entailments = []\n",
    "for q in tqdm(yesno):\n",
    "    outputs = []\n",
    "    sentences = [i['text'] for i in q.ranked_sentences()]\n",
    "    s1, s2 = sentences, [q.assertion_pos] * len(sentences)\n",
    "    total = 0\n",
    "    for i in range(len(s1)):\n",
    "        ss1 = [['<s>'] + [word for word in word_tokenize(sent) if word in word_vec] +  ['</s>'] for sent in [s1[i]]]\n",
    "        ss2 = [['<s>'] + [word for word in word_tokenize(sent) if word in word_vec] +  ['</s>'] for sent in [s2[i]]]\n",
    "        s1_batch, s1_len = get_batch(ss1, word_vec)\n",
    "        s2_batch, s2_len = get_batch(ss2, word_vec)\n",
    "        s1_batch, s2_batch = Variable(s1_batch), Variable(s2_batch)\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "        total += output\n",
    "        outputs.append(output.data.max(1)[1].numpy()[0])\n",
    "    n_entail = (np.array(outputs) == 0).sum()\n",
    "    n_contradict = (np.array(outputs) == 2).sum()\n",
    "    total_res = total.data.max(1)[1].numpy()[0] if isinstance(total, Variable) else 0\n",
    "    q.exact_answer = get_answer(n_entail, n_contradict)\n",
    "    entailments.append((n_entail, n_contradict, total_res, q.exact_answer))\n",
    "\n",
    "data.eval_yesno()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sonidegib phosphate: new approval for basal cell carcinoma',\n",
       " 'Sonidegib, a novel smoothened inhibitor for the treatment of advanced basal cell carcinoma',\n",
       " 'Sonidegib (Odomzo), an oral smoothened (SMO) antagonist, is indicated for the treatment of adults with locally advanced basal cell carcinoma (laBCC) who are not candidates for surgery or radiation therapy, or adults with recurrent laBCC following surgery or radiation therapy',\n",
       " 'Oral sonidegib is approved in Switzerland for the treatment of adult patients with advanced basal cell carcinoma (BCC) and in the US and EU for the treatment of adult patients with locally advanced BCC that has recurred following surgery or radiation therapy, or those who are not candidates for surgery or radiation therapy',\n",
       " 'Oral Smoothened (Smo) inhibitors Vismodegib, Sonidegib, and Taladegib have shown to be effective in several trials',\n",
       " 'The hedgehog pathway inhibitor sonidegib demonstrated meaningful tumor shrinkage in more than 90% of patients with locally advanced basal cell carcinoma (BCC) or metastatic BCC in the BCC Outcomes with LDE225 Treatment study.This report provides long-term follow-up data collected up to 12months after the last patient was randomized.In this multicenter, randomized, double-blind phase II study, patients were randomized 1:2 to sonidegib 200 or 800mg',\n",
       " 'Serious adverse events occurred in 11 (14%) of 79 patients in the 200 mg group and 45 (30%) of 150 patients in the 800 mg group.The benefit-to-risk profile of 200 mg sonidegib might offer a new treatment option for patients with advanced basal cell carcinoma, a population that is difficult to treat.Novartis Pharmaceuticals Corporation.<CopyrightInformation>Copyright  2015 Elsevier Ltd. All rights reserved.</',\n",
       " 'Serious adverse events occurred in 11 (14%) of 79 patients in the 200 mg group and 45 (30%) of 150 patients in the 800 mg group.The benefit-to-risk profile of 200 mg sonidegib might offer a new treatment option for patients with advanced basal cell carcinoma, a population that is difficult to treat.Novartis Pharmaceuticals Corporation.<CopyrightInformation>Copyright  2015 Elsevier Ltd. All rights reserved.</C',\n",
       " 'However, to date, Hh inhibitors, specifically those targeting Smoothened [such as vismodegib, BMS-833923, saridegib (IPI-926), sonidegib/erismodegib (LDE225), PF-04449913, LY2940680, LEQ 506, and TAK-441], have demonstrated good efficacy as monotherapy in patients with basal cell carcinoma and medulloblastoma, but have shown limited activity in other tumor types',\n",
       " 'Sonidegib is a new smoothened inhibitor currently under investigation for treatment of laBCC, which demonstrates a comparable safety profile to vismodegib',\n",
       " 'The recent development of novel hedgehog pathway inhibitors for high-risk BCC (including oral vismodegib and sonidegib) may represent a paradigm shift towards medical management of NMSC',\n",
       " 'The acceptable benefit-risk profile of sonidegib, along with a paucity of treatment options and the seriousness of the condition, makes sonidegib an emerging option for the treatment of adults with laBCC that has recurred following surgery or radiation therapy, or in those who are not candidates for surgery or radiation therapy',\n",
       " 'This review of the literature aims to describe previous and current treatment options for oral therapy in locally advanced and metastatic NMSC otherwise unamenable to standard treatment',\n",
       " 'The primary end point was objective response rate assessed by central review.Objective response rates in the 200- and 800-mg arms were 57.6% and 43.8% in locally advanced B']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nitish/Documents/Box Sync/cmu/acads/11797/bioasq\n"
     ]
    }
   ],
   "source": [
    "cd ~/acads/11797/bioasq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.save_yesno_answers(yesno_ans_file)\n",
    "# data.load_answers_from_file(yesno_ans_file, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there an association between Klinefelter syndrome and breast cancer?\n",
      "yes\n",
      "Male breast cancer risk factors show strong association with BRCA2 mutations, as well as Klinefelter syndrome\n",
      "Screening for breast cancer in male-to-female transsexuals should be undertaken for those with additional risk factors (e.g., family history, BRCA2 mutation, Klinefelter syndrome) and should be available to those who desire screening, preferably in a clinical trial\n",
      "Patients with 47, XXY karyotype (Klinefelter syndrome) appear to have increased risk of developing cancer, especially male breast cancer, germ cell tumours and non Hodgkin lymphomas, but rarely acute myeloid leukaemia\n",
      "Major risk factors for developing male BC include clinical disorders involving hormonal imbalances (excess of estrogen or a deficiency of testosterone as seen in patients with Klinefelter syndrome) and a positive family history for breast cancer\n",
      "The main risk factors include: the mutation of genes BRCA 1 and 2, Klinefelter's syndrome, alcohol, liver disease, obesity\n",
      "Klinefelter syndrome (OR = 24.7; 95% CI = 8.94 to 68.4) and gynecomastia (OR = 9.78; 95% CI = 7.52 to 12.7) were also statistically significantly associated with risk, relations that were independent of BMI\n",
      "Although aetiology is still unclear, constitutional, environmental, hormonal (abnormalities in estrogen/androgen balance) and genetic (positive family history, Klinefelter syndrome, mutations in BRCA1 and specially BRCA2) risk factors are already known\n",
      "The largest study found 19.2- and 57.8-fold increases in incidence and mortality, respectively, with particularly high risks among 47,XXY mosaics\n",
      "CONCLUSIONS: Additional well-designed epidemiologic studies are needed to clarify which patients with KS are at a high risk of developing MBC and to distinguish between possible predisposing factors, including altered endogenous hormones\n"
     ]
    }
   ],
   "source": [
    "q = yesno[5]\n",
    "print q.question\n",
    "print q.exact_answer\n",
    "for i in q.ranked_sentences():\n",
    "    print i['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
